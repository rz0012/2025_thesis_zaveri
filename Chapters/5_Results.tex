\chapter{Results} % Main chapter title

\label{Chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{figures/Qualitative_BBBC006}
\caption{\textbf{BBBC006 dataset.} Qualitative results on an image from the BBBC006 dataset at different levels of focus. $\mathtt{z}$=16 is considered in-focus, while values further from this are more out of focus in either direction. Segmentations are shown in gray with cell borders shown in blue. AP values are computed at 0.5 IoU.}
\label{fig:qualitative_results}
\end{figure*}

%\section{Experiments}

\noindent
We evaluate the proposed approach with several datasets and multiple settings. Section~\ref{general_results} presents a series of comparisons with generalist and unsupervised domain adaptation models on multiple datasets. We note that although many other instance segmentation methods have been proposed, the selected comparison approaches used within each experiment represent the state-of-the-art in their respective niche; namely, ``generalist'' (pretrained) and unsupervised approaches. Experiments using a 3-D segmentation extension of CellTranspose are detailed in Section~\ref{3D_results}. Finally, we examine different components of our approach with an ablation study in Section~\ref{ablation_results}.

In absence of additional direction, the following setup is used for each experiment. The model in Section~\ref{sec-pretrained-model} is pretrained with the ``generalized'' dataset of~\cite{Stringer2021-yw} as source data $\mathcal{D}^s$. This dataset is composed of samples from 6 different datasets and image sources, totalling 540 images in the training set and 68 in the test set. Adaptation is done as described in Section~\ref{sec-adaptation}. We find that SGD with initial learning rate $10^{-2}$, momentum 0.9, weight decay $10^{-5}$, and batch size of 2 provides optimal results. For the first five epochs, the learning rate decreases by a factor of 10 each epoch, and is kept constant for the remaining five. We take square patches with side length $h=w=112$, use a minimum overlap of $84$ during evaluation, and enforce a nominal cell size $m_n=30$. Additional hyperparameters are set as: $| \mathcal{N}_i |=20$, $\tau=0.1$, $m=10$, $\lambda=1$, $\gamma_1=0.05$, $\gamma_2=2$, and $\delta=0.05$. Because the source dataset used is always significantly larger than the target data, adaptation takes roughly the same amount of time regardless of the size of $K$ or the target data to which the model is adapted. Using a singular NVIDIA TITAN Xp GPU, adaptation takes approximately 5 minutes to complete for each experiment.

%Argument for not showing other segmentation approaches - NuSeT will effectively always underperform Cellpose, same with Mask-RCNN and vanilla U-Net, as shown in Cellpose paper. ``We primarily focus on Cellpose due to the following reasons:''

\section{General Results} \label{general_results}

Details of each dataset and the method utilized for creating splits of data are described below. In general, we follow the data splitting guidelines laid out by previous works to ensure fair comparisons. In order to fit our approach to these guidelines, we draw our $K$ sample patches, to produce $\mathcal{D}^t_K$, from the training split of the dataset representing the target distribution. Instead, the testing split of the target distribution dataset is used as our remaining, unlabeled portion of the target dataset $\mathcal{D}^t$. Thus, the size of each target training set doesn't affect our approach, although it serves as a reference for the amount of data available to the fully trained and unsupervised methods we compare against.

\subsection{Broad Bioimage Benchmark Collection -- 006}
We evaluated CellTranspose on the target dataset BBBC006~\cite{Ljosa2012-si}, hosted by the Broad Institute. This dataset is composed of human U2OS cells which are fairly homogeneous and easy to segment in ideal settings. However, the same tissue samples have been imaged with different focus settings, generating different images, which allow us to observe the effect of the associated covariate shift on the generalist model as well as our approach. BBBC006 is actually composed of multiple sets containing images of the same experiment from varying levels of focus. In its entirety, there are 34 separate versions of the data -- $z=00$ through $z=33$. As $z=16$ is considered to be in-focus, we select datasets at two distinct distances from the ideal setting in either direction, resulting in our using the datasets $z=00$, $z=08$, $z=16$, $z=24$, and $z=32$.

BBBC006 contains images from an experiment using a 384-well microplate with two sites per well. Therefore, when both image channels (originally contained as separate files) are combined, this results in 768 2-channel images. Each image is named based on its position within the $16 \times 24$-well microplate - thus, we arbitrarily select the first three columns (01-03) to be test data and use the remaining samples for training. This results in 672 images within the training set and 96 within the test set -- or 87.5\% and 12.5\% of the dataset, respectively.

\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
\caption{\textbf{Numerical results on BBBC006 dataset.} Average Precision is measured at IoU threshold 0.5. Best and second best performance are indicated in \textbf{bold} and \underline{underline}, respectively}
\label{table:bbbc006_quant}
%\scalebox{0.75}{
\begin{tabular}{l|lllll}
\hline\noalign{\smallskip}
{\bf BBBC006 Results} & $z=00$ & $z=08$ & $z=16$ & $z=24$ & $z=32$ \\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
{\bf Cellpose} 					& 0.233 & 0.814 & 0.770 & 0.779 & 0.406\\
{\bf CellTranspose (1-shot)} 	& 0.320 & 0.790 & 0.854 & 0.829 & 0.459\\
{\bf CellTranspose (2-shot)} 	& 0.399 & 0.831 & \underline{0.858} & 0.838 & \underline{0.525}\\
{\bf CellTranspose (3-shot)} 	& 0.509 & \textbf{0.837} & 0.851 & \underline{0.841} & 0.502\\
{\bf CellTranspose (5-shot)} 	& \underline{0.531} & 0.834 & \textbf{0.862} & \textbf{0.842} & \textbf{0.557}\\
{\bf CellTranspose (10-shot)} 	& \textbf{0.581} & \underline{0.835} & 0.857 & 0.833 & 0.486\\
\hline
\end{tabular}
%}
\end{center}
%\vspace{5mm}
\end{table}

\begin{figure*}
\centering
% \includegraphics[height=5cm]{figures/BBBC006_AP_Graphs.eps}
\includegraphics[height=8cm]{figures/AP_Graphs/z=00/Results_z=00}
\caption{\textbf{Results on BBBC006 at z=00.} Average precision of $K$-shot adaptation with $K= 1, 2, 3, 5, 10$ target samples, compared with the original Cellpose pretrained model.}
\label{fig:bbbc_qual_00}
\end{figure*}

\begin{figure*}
\centering
% \includegraphics[height=5cm]{figures/BBBC006_AP_Graphs.eps}
\includegraphics[height=8cm]{figures/AP_Graphs/z=08/Results_z=08}
\caption{\textbf{Results on BBBC006 at z=08.} Average precision of $K$-shot adaptation with $K= 1, 2, 3, 5, 10$ target samples, compared with the original Cellpose pretrained model.}
\label{fig:bbbc_qual_08}
\end{figure*}

\begin{figure*}
\centering
% \includegraphics[height=5cm]{figures/BBBC006_AP_Graphs.eps}
\includegraphics[height=8cm]{figures/AP_Graphs/z=16/Results_z=16}
\caption{\textbf{Results on BBBC006 at z=16 (in-focus).} Average precision of $K$-shot adaptation with $K= 1, 2, 3, 5, 10$ target samples, compared with the original Cellpose pretrained model.}
\label{fig:bbbc_qual_16}
\end{figure*}

\begin{figure*}
\centering
% \includegraphics[height=5cm]{figures/BBBC006_AP_Graphs.eps}
\includegraphics[height=8cm]{figures/AP_Graphs/z=24/Results_z=24}
\caption{\textbf{Results on BBBC006 at z=24.} Average precision of $K$-shot adaptation with $K= 1, 2, 3, 5, 10$ target samples, compared with the original Cellpose pretrained model.}
\label{fig:bbbc_qual_24}
\end{figure*}

\begin{figure*}
\centering
% \includegraphics[height=5cm]{figures/BBBC006_AP_Graphs.eps}
\includegraphics[height=8cm]{figures/AP_Graphs/z=32/Results_z=32}
\caption{\textbf{Results on BBBC006 at z=32.} Average precision of $K$-shot adaptation with $K= 1, 2, 3, 5, 10$ target samples, compared with the original Cellpose pretrained model.}
\label{fig:bbbc_qual_32}
\end{figure*}

Figure~\ref{fig:qualitative_results} shows qualitative segmentation results. As we move away from the optimal focal plane the generalist Cellpose model exposes greater performance deterioration than CellTranspose.

We also test CellTranspose with different numbers $K= 1, 2, 3, 5, 10$, of few-shot adaptation. Figures~\ref{fig:bbbc_qual_00}-\ref{fig:bbbc_qual_32} plot the average precision (AP) against the intersection over union (IoU) for the different focal settings. CellTranspose consistently achieves high performance levels with as few as three annotated shots. Beyond this, results begin to exhibit diminished returns. Therefore, unless otherwise specified, other results have been obtained with a 3-shot adaptation, given the balance it gives between performance and annotation requirement.

We additionally share the precise values obtained for BBBC006 for an average precision at intersection-over-union (IoU) threshold 0.5, shown in Table~\ref{table:bbbc006_quant}.




%\setlength{\tabcolsep}{4pt}
%\begin{table}
%\begin{center}
%\caption{Average Precision of few-shot adaptation with different numbers of target samples. Experiments were completed using five BBBC006 datasets. AP is calculated for an IoU threshold of 0.5}
%\label{table:bbbc006_gen}
%\begin{tabular}{l|lllll}
%\hline\noalign{\smallskip}
%{\bf Few-shot Results} & z=00 & z=08 & z=16 & z=24 & z=32\\
%\noalign{\smallskip}
%\hline
%\noalign{\smallskip}
%%Cellpose (cyto model) & 0.0318 & 0.501 & \textbf{0.862} & 0.767 & 0.079\\
%Cellpose (nuclei model)  & 0.228 & 0.810 & 0.765 & 0.773 & 0.393\\
%CellTranspose (1-shot)  & 0.068 		 & 0.796 & 0.832 & 0.815 & 0.463\\
%CellTranspose (2-shot)  & 0.206 		 & \textbf{0.840} & 0.828 & \underline{0.831} & 0.607\\
%CellTranspose (3-shot)  & 0.494 		 & \underline{0.837} & 0.849 & \underline{0.831} & 0.643\\
%CellTranspose (5-shot)  & \underline{0.519} 		 & 0.831 & \textbf{0.852} & 0.818 & \textbf{0.715}\\
%CellTranspose (10-shot) & \textbf{0.557} & 0.830 & \underline{0.851} & \textbf{0.832} & \underline{0.705}\\
%\hline
%\end{tabular}
%\end{center}
%\end{table}
%\setlength{\tabcolsep}{1.4pt}


\subsection{TissueNet}
We evaluated CellTranspose on TissueNet~\cite{Greenwald2021-uj}, a dataset developed alongside a generalist method called Mesmer. TissueNet is comprised of samples from various imaging platforms and tissue types, providing a wide-spanning array of cellular images. In~\cite{Greenwald2021-uj} one set of experiments split TissueNet into subsets of the four most common imaging types, each of which was further divided into different tissue types. Similarly, four other subsets were composed of the four most common tissue types, each being further split into the imaging types that make up the samples for that tissue type.

Each experiment contained within the dataset was split in a manner that resulted in 80\% of the data being used for training and 10\% for both validation and testing. TissueNet contains both nuclei and whole-cell segmentation labels: for the purposes of our work, we train using the nuclei segmentations.
Table~\ref{table:tissuenet} shows the $F_1$-scores on these eight data splits, computed for the generalist Cellpose, Mesmer trained on each of the splits, and CellTranspose 3-shot adapted to each of the splits. Again, CellTranspose consistently shows improved performance. 

For each of the experiments completed in this work, the performance of our approach is compared to that of the generalized Cellpose model, which operates as a lower bound as it represents pre-adaptation. As~\cite{Greenwald2021-uj} reports that the fully-trained Cellpose model achieves a level of performance rivaling that of their in-house segmentation technique on TissueNet, the results of Mesmer may appear to serve as an upper bound in this scenario. However, as briefly described in \textbf{Section 5.1}, the ``specialist'' models they use are trained for each of the eight subsets selected from TissueNet. In our case, we train on \emph{subsets of each subset}. For example, the samples of breast tissue were obtained using three distinct imaging platforms: IMC, MIBI-TOF, and Vectra. In regards to the intended use of CellTranspose, each of these three platforms would correspond to individual experiments. Because of this, we train on each of these subsets of the breast tissue subset and combine the results at the end to obtain the corresponding $F_1$-score that we report. It should be noted that~\cite{Greenwald2021-uj} describes that their primary generalized model performs approximately as well as each specialist model for the given subsets. However, we believe it to be the case that only when exposed to a singular domain, as opposed to the two to five domains each specialist model is trained on, does specialization truly yield an increase in performance -- our results support this claim.

In contrast to the AP results used in the other experiments, we report our performance in terms of $F_1$-score and compute it in the same manner as is done in~\cite{Greenwald2021-uj}. In particular, \cite{Greenwald2021-uj} considers true positives to be any predicted segmentation that intersects with a ground truth segmentation; cases where multiple overlaps occur are settled by selecting the ground truth-prediction pair that contains the greatest IoU, repeating the process until no such pairs remain. The authors further distinguish between false positive and false negative cases including the categories ``merge,'' ``split,'' and ``other,'' but these definitions are not required for the purposes of this calculation.

We report a sampling of qualitative results on TissueNet in Figure~\ref{fig:qualitative_TissueNet}.

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figures/Qualitative_TissueNet}
\caption{\textbf{Qualitative results on images from the \emph{TissueNet} dataset.} Samples were selected such that all four most common tissue types as well as all four most common imaging types were represented. Segmentations in each sample are shown in gray with cell borders shown in blue}
\label{fig:qualitative_TissueNet}
\end{figure*}

%\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
\caption{\textbf{TissueNet dataset.} $F_1$-score from state-of-the-art ``generalist'' approaches and CellTranspose.}
\label{table:tissuenet}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{l|llll|llll}
\hline\noalign{\smallskip}
{\bf TissueNet} & \multicolumn{4}{c|}{\textbf{Platform-specific}} & \multicolumn{4}{c}{\textbf{Tissue-specific}} \\
{\bf Results} & CODEX & CyCIF & MIBI & Vectra & Breast & GI & Imm. & Panc.\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
Cellpose (Gen.)	& 0.785	& 0.548 & 0.479 & 0.609 & 0.670 & 0.523 & 0.350 & 0.797\\
Mesmer					& 0.88 	& 0.80	& 0.76	& 0.72	& 0.74	& 0.82	& 0.82	& 0.92\\
CellTranspose           & \textbf{0.940} & \textbf{0.940} & \textbf{0.932} & \textbf{0.918} & \textbf{0.911} & \textbf{0.906} & \textbf{0.934} & \textbf{0.955}\\
\hline
\end{tabular}
\end{adjustbox}
\end{center}
\vspace{-5mm}
\end{table}
%\setlength{\tabcolsep}{1.4pt}

% Although numerical results were not provided in the original paper, Cellpose and StarDist were reported to achieve equivalent performance to that of Mesmer when trained directly on TissueNet data, all three of which rivaled the accuracy of expert human annotators. Thus, the results of Mesmer provide a solid comparison. See Table~\ref{table:tissuenet}.
%while also suggesting that if trained on a different dataset, Mesmer would likely perform at the same levels as the "generalist" Cellpose and StarDist models.


\subsection{Triple Negative Breast Cancer}
Among the most challenging types of cellular data to segment is that of hematoxylin and eosin-stained (H\&E) images. This is in part due to the fact that multiple cell types often appear within an individual sample, in addition to the high variability of the background. The Triple Negative Breast Cancer (TNBC) dataset~\cite{Naylor2019-ak}, gathered by the Curie Institute, is comprised of 50 images obtained from 11 distinct tissue types, furthering the inherent difficulty of the dataset.

We compare CellTranspose with the top-performing unsupervised cellular instance segmentation approach, CyC-PDAM~\cite{Liu2020-qh}. Following the lead of the authors of this work, the dataset is split into 40 images for training, using 8 of the tissue types. We adapt our models using 3, 5, and 10 sample patches selected from the training dataset. Each model is then tested on the 10 images from the remaining 3 tissue types. From this description of the dataset split, this leaves some ambiguity as to which tissue types should belong to training and which should belong to testing. Thus, we select the test tissue types in a random manner, ensuring that the above conditions of image and tissue count per split are met. Results, using the same metrics as CyC-PDAM, are shown in Table~\ref{table:tnbc}, where it can be noted that a 5-shot adaptation leads to performance metrics comparable with those of CyC-PDAM, which has used all the training source data available to adapt the model in an unsupervised manner.

It should be noted that using the TNBC dataset in this manner is outside of the intended use case of our approach. Generally, \emph{CellTranspose} is expected to be used for adapting to each new ``experiment,'' which in practice consists of a singular tissue type gathered by the biologist. In this way, our approach provides a model finely tuned to the given experiment while requiring minimal annotation. However, in order to compare our method directly to CyC-PDAM, we construct this test as closely to their procedure as possible at a consequent disadvantage for CellTranspose.

\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
\caption{\textbf{TNBC dataset.} Comparison between the top unsupervised approach and CellTranspose. Best results are in bold, and second best results are underlined.}
\label{table:tnbc}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{l|l|l|l}
\hline\noalign{\smallskip}
{\textbf{BBBC039 $\rightarrow$ TNBC}} & AJI & Pixel-$F_1$ & Object-$F_1$ \\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
CyC-PDAM~\cite{Liu2020-qh}						& 0.5672 $\pm$ 0.0646
												& \textbf{0.7593 $\pm$ 0.0566}
												& 0.7478 $\pm$ 0.0417 \\
CellTranspose 3-shot 						& 0.4916 $\pm$ 0.0771
												& 0.6702 $\pm$ 0.0710
												& 0.7092 $\pm$ 0.0818 \\
CellTranspose 5-shot 						& \underline{0.5693 $\pm$ 0.0576} 
												& 0.7377 $\pm$ 0.0431
												& \underline{0.7825 $\pm$ 0.0625} \\
CellTranspose 10-shot						& \textbf{0.5906 $\pm$ 0.0617}
												& \underline{0.7568 $\pm$ 0.0493} 
												& \textbf{0.7879 $\pm$ 0.0687} \\
\hline
\end{tabular}
\end{adjustbox}
\end{center}
\vspace{-5mm}
\end{table}
\setlength{\tabcolsep}{1.4pt}





\section{3-D Segmentation} \label{3D_results}

%%% Fill this out

As biological applications often produce and require the analysis of 3-D data, having a segmentation technique that can operate on volumetric data is desirable. Following the approach in~\cite{Stringer2021-yw}, we extend CellTranspose to operate in 3-D using the same initial pretrained 2-D model and the same source dataset for training and adaptation. During adaptation, 2-D slices of the data, taken from multiple positions of a volumetric patch in order to provide a greater invariance to cell cross-sectional area, are gathered and used as the few-shot target dataset. The model then makes predictions on each 2-D slice within the test dataset. The predictions are made along the $xy$, $yz$, and $zx$ planes, producing three distinct predictions each consisting of the foreground-background mask and two gradient flows along the directions of the given plane. These predictions are subsequently averaged together to generate the final output prediction such that it contains \emph{three}-dimensional gradient flows. For instance, a pixel $i$'s final output representation would consist of a foreground-background prediction averaged from the corresponding foreground-background prediction of $i$ in the $xy$, $yz$, and $zx$ planes, an $x$-flow averaged from the $xy$ and $zx$ plane, a $y$-flow averaged from the $xy$ and $yz$ plane, and a $z$-flow averaged from the $yz$ and $zx$ plane.

We test our approach on two 3-D datasets. \textbf{BBBC024} is composed of synthetic annotated human U2OS cells, similar to those within BBBC006, from the Broad Institute. BBBC024 contains even amounts of images across clustering probabilities of 0\%, 25\%, 50\%, and 75\%, as well as low and high signal-to-noise ratio (SNR). Images were split such that there would be a balanced contribution to training, validation, and testing both within each clustering/SNR scenario as well as across the entire dataset. Training and validation splits together comprised 80\% of the data, which was further split into 80\% training and 20\% validation. In all, training contained approximately 64\% of the images, validation 16\%, and testing 20\%, or 154, 38, and 48 images each.

The \textbf{Worm} dataset is a series of nuclei images from larval stage C. elegans. We follow the same protocol for the Worm dataset as is used in StarDist-3D~\cite{Weigert_2020_WACV}. We randomly split the 28 volumes into 18, 3, and 7 for training, validation, and testing, respectively.
Unlike other pretrained approaches, StarDist-3D~\cite{Weigert_2020_WACV} has been trained directly on the dataset in question, and serves here as an upper bound. Table~\ref{table:3D_results} shows the average precision results, highlighting the improvement of CellTranspose over the generalist Cellpose3D. Figures~\ref{fig:BBBC024_segmentation} and~\ref{fig:Worm_segmentation} provide a qualitative comparison between 3D segmentations of the BBBC024 and Worm datasets, respectively.


\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/Qualitative_BBBC024_2_fast}
\caption{\textbf{3D Segmentation.} Qualitative segmentation results on samples from the BBBC024 dataset. Note that Cellpose3D tends to oversample; this is likely due to the fact that the particular characteristics of the data Cellpose3D was trained on necessitated pixels of even lower intensities to be segmented. More importantly, on the bottom right of the results obtained with Cellpose3D, it is possible to observe some over-segmentation effects, which are not present in the results obtained with the proposed CellTranspose. Segmentation visualizations were produced using the napari library~\cite{Sofroniew2022-kk}.}
\label{fig:BBBC024_segmentation}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/Qualitative_Worm_fast}
\caption{\textbf{3D Segmentation on Worm dataset.} Similar to Figure~\ref{fig:BBBC024_segmentation}, qualitative results, this time on the Worm dataset, are shown. Instead of Cellpose, we compare our results to StarDist-3D. Segmentations appear relatively similar, although segmentation accuracy (shown in Table~\ref{table:3D_results}) is better for StarDist-3D, which was directly trained using training images from the Worm dataset. With the knowledge that average precision is improved for the approach fully trained using data from the domain evaluated, it should be noted that StarDist-3D's approach results in more spherical shapes; some of the detail of the cell shape is lost, and StarDist would likely experience a loss in performance on cells with more complex morphologies.}
\label{fig:Worm_segmentation}
\end{figure}

%\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
\caption{\textbf{3D datasets.} Average Precision at IoU threshold 0.5.}
\label{table:3D_results}
%\begin{adjustbox}{width=0.5\columnwidth}
\begin{tabular}{l|l|l}
\hline\noalign{\smallskip}
{\bf 3-D Results} & {\bf Worm} & {\bf BBBC024}\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
StarDist-3D				& 0.765 	& --- \\
Cellpose3D			    & 0.575 	& 0.822 \\
CellTranspose  			& 0.648 	& 0.994 \\
\hline
\end{tabular}
%\end{adjustbox}
\end{center}
\vspace{-5mm}
\end{table}
%\setlength{\tabcolsep}{1.4pt}





\section{Ablation Study} \label{ablation_results}

% The results of our ablation study are shown below. As our primary contributions are the development of two contrastive loss functions and an improved cell dimension calculation mechanism, we focus on these here.

Table~\ref{table:ablation} shows ablation results computed on the BBBC006 dataset with $z$=00 and 3-shot adaptation. The addition of both contrastive losses improves the overall AP by more than 6\%. Interestingly, however, the removal of only one adaptation loss tends to decrease the performance to below that of the direct training approach, which follows the same training scheme but without either adaptation loss. This indicates that the flow and mask losses are intrinsically tied to one another, which is consistent with the fact that weights are shared between both outputs until the final layer.

Additionally, the cell diameter calculation method seems to play an important role in accurate segmentation. As the original Cellpose approach computes the cell diameter based upon the number of pixels corresponding to a particular cell, non-spherical cells have the potential to provide a similar diameter while spanning a much larger area. Thus, it is coherent that computing the total rectangular area enclosing a cell yields a more appropriate resizing approach, which is how it is done in CellTranspose.
%We additionally detail the results regarding experiments with the temperature hyperparameter used in the Contrastive Flow Loss. This parameter represents a tradeoff between embedding distribution uniformity as well as the tolerance toward semantically similar samples, which in turn describes the need for balance between training time and performance. The given setting provides unique characteristics regarding the negative sample distribution and the application of the loss function directly onto the output, making this is particularly interesting to study. *Possibly others? Possibly not even this one?*

% \setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
\caption{\textbf{Ablation results on BBBC006.} AP is calculated for an IoU threshold of 0.5.}
\label{table:ablation}
%\begin{adjustbox}{width=0.7\columnwidth}
\begin{tabular}{l|l}
\hline\noalign{\smallskip}
{\bf Ablation Results} & $AP_{50}$\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
CellTranspose & \textbf{0.509}\\
Without Flow Adaptation & 0.414\\
Direct Training (no Adaptation Losses) & 0.441\\
Using Cellpose cell diameter calculation & 0.390\\
%CellTranspose & 0.4935155212879181\\
%Without Flow Adaptation & 0.4137983024120331\\
%Direct Training (no Adaptation) & 0.4405716359615326\\
%CellTranspose with cell diameter calculation & 0.39014795422554016\\
\hline
\end{tabular}
%\end{adjustbox}
\end{center}
\vspace{-5mm}
\end{table}
% \setlength{\tabcolsep}{1.4pt}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
