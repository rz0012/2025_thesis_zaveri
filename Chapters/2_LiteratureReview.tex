\chapter{Literature Review} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Visual Object Tracking}
Visual single object tracking is a significant challenge in the vision community. This is due to the many factors affecting the target image variability, like camera and object motion, motion blur, resolution changes, pose and deformation changes, occlusion and visibility changes. Many of the current benchmark datasets explore these challenges, and~\cite{noman2022avist} is a good example. Traditional approaches have used discriminative correlation filters~\cite{henriques2014high, ding2017real, marvasti2019rotation, marvasti2021adaptive, marvasti2021efficient}, silhouette tracking~\cite{xiao2016efficient}, kernel~\cite{bruni2014improvement}, and point tracking~\cite{lychkov2018tracking} using engineered feature extractors such as HOG filters~\cite{dalal2005histograms} and Color-Names~\cite{van2007learning}. Even if based on effective hand-crafted feature extractors, their performance deteriorate quickly when facing significant changes in relative motion or appearance~\cite{marvasti2021deep}. The recent deep learning based approaches have proven to be much more robust to those changes, especially beginning from those based on the siamese architectures~\cite{koch2015siamese,melekhov2016siamese}. 

\section{Siamese-based Object Trackers}
Siamese-based object trackers make use of offline training and perform template-matching to localize the object in the current frame. Unlike online object trackers, these networks are very efficient, achieving a tracking speed of over 100 FPS on a single GPU, as we first observed in GOTURN~\cite{held2016learning} and SiamFC~\cite{bertinetto2016fully}. Further, SiamRPN~\cite{li2018high} introduced a region proposal based tracker and DaSiamRPN~\cite{zhu2018distractor} further extended this idea to implement distractor aware siamese networks. Next, UpdateNet~\cite{zhang2019learning} introduced the idea of learning the update of the dynamically changing object. SiamRPN++~\cite{li2019siamrpn++} and SiamFC++~\cite{xu2020siamfc++} further improved the anchor-based and anchor-free methods~\cite{bertinetto2016fully} respectively. Diverging from the other siamese-based prediction approaches, SiamMask~\cite{wang2019fast} also performs segmentation with object tracking, while SiamCAR~\cite{guo2020siamcar} and SiamBAN~\cite{chen2020siamese} introduce a fully convolutional framework for object tracking without anchors. More recently, discriminative classifier~\cite{mayer2021learning} and transformer~\cite{yan2021learning, cui2022mixformer} based approaches have shown significant improvement over the siamese-based networks; however, in this work, we show that siamese-based networks can still be effective when integrated with novel design. 


\section{Transformer-based Object Trackers}
Since transformer networks were proposed~\cite{vaswani2017attention, dosovitskiy2020image}, there has been a greater use of the attention-based trackers. AiATrack~\cite{gao2022aiatrack} incorporates an internal attention module inside the attention block to enhance the correlations of each query-key pair. ARTrack~\cite{wei2023autoregressive} and SeqTrack~\cite{chen2023seqtrack} approach the tracking issue by incorporating sequential predictions of the bounding boxes and achieve SOTA results. A simpler and more effective approach, TATrack~\cite{he2023target}, utilizes intermediate frame information and introduces a long-term contextual attention module to attend to multiple-frames at once, achieving SOTA accuracy. Prior to this, STARK~\cite{yan2021learning} proposed a transformer network working in a siamese-fashion and takes advantage of the intermediate frames during inference. Other Siamese-based approaches~\cite{yu2020deformable, yang2020siamatt} have also explored attention for these networks; however, they render less effective compared to their Transformer counterparts. 


\section{Efficient Tracking}
Most SOTA trackers~\cite{gao2022aiatrack,he2023target,yan2021learning,wei2023autoregressive,chen2023seqtrack} perform very well on current object tracking benchmarks~\cite{Huang2021,fan2021lasot, kiani2017need, muller2018trackingnet, 7001050, mueller2016benchmark}. However, they fall short on a few very significant issues: speed, scalability, and hardware they operate on. Having enormous amount of parameters and performing hundreds of Giga FLOPs is affordable only on high-end computers, and it is not tailored to applications ``in the wild.'' Therefore, the pursuit of an accurate lightweight tracker is still an important challenge. Initially, SqueezeNet~\cite{iandola2016squeezenet} introduced a very efficient network achieving an accuracy similar to the AlexNet backbone~\cite{krizhevsky2012imagenet}. More recently, \cite{wan2020fbnetv2, howard2019searching} introduced FBNet and MobileNet involving separable convolutional layers, which are extremely efficient to run on mobile devices. Additionally, LightTrack~\cite{yan2021lighttrack} used neural architecture search (NAS)~\cite{pham2018efficient} to design a lightweight tracker that  performs quite well. Further, NanoTrack~\cite{githubGitHubHonglinChuNanoTrack} utilizes MobileNetV3~\cite{howard2019searching} as backbone and uses an approach similar to~\cite{chen2020siamese,yan2021lighttrack} to enable compatibility with mobile devices. FEAR~\cite{borsuk2022fear} incorporates these findings and introduces an extremely efficient lightweight tracker that could easily be integrated with any hardware, including mobile devices, and achieved very good performance. We follow their footsteps, and design an efficient tracker that only uses two convolutional layers in the regression head with a lower spatial resolution.\\

On the other hand, Transformer-based approaches\cite{gao2022aiatrack, wei2023autoregressive,chen2023seqtrack, yan2021learning} show reasonable accuracy, however, they lack efficiency as they utilize computationally heavy attention layers. 
To alleviate that, E.T.Track~\cite{blatter2023efficient} incorporates an efficient Exemplar Transformer block on the prediction heads. While HCAT~\cite{chen2022efficient} uses multiple hierarchical cross-attention blocks with feature sparsification, HiT\cite{kang2023exploring}, instead, leverages a lightweight hierarchical transformer backbone to achieve improved accuracy and speed. MixformerV2~\cite{cui2024mixformerv2} uses distillation to reduce the number of FLOPs, whereas SMAT \cite{gopal2024separable} uses separable mixed attention to maintain the accuracy on their one-stream transformer networks. Since these are smaller networks, they have limited representational capacity and do not generalize well to OOD sets, making them less reliable for tracking ``in-the-wild", which we tackle in the proposed framework.


\section{Efficient Attention}
\cite{mehta2022separable} proposed separable transformer blocks with convolutional layers to increase efficiency while maintaining accuracy. MobileViTv3~\cite{wadekar2022mobilevitv3} further replaced heavy transformer blocks with their CNN-ViT-based separable attention blocks, and showed considerable performance gain. Originally, CBAM~\cite{woo2018cbam}, DANet~\cite{fu2019dual}, and Polarized Self-Attention~\cite{liu2021polarized} explored attention in convolution by computing channel and spatial attentions separately. CBAM~\cite{woo2018cbam} and PSA~\cite{liu2021polarized} further incorporate a squeeze-and-excite framework~\cite{iandola2016squeezenet} to excite relevant features across the channels. The latter is a more powerful and efficient variant of CBAM.
This suggests that separability is inevitable for efficient attention blocks; therefore, in this work we propose a simplified fast convolution-based separable attention framework.

\section{Correspondence Learning}
A little less explored area within tracking is correspondence learning. Most approaches make use of pre-trained backbones on ImageNet \cite{deng2009imagenet}. They further freeze certain layers and fine tune the rest of the model; however, one caveat here is that the pre-trained models are typically trained on a specific supervised task and do not possess proper distributions of the representations it has learned for the other tasks, for example, a model pre-trained on ImageNet classification would experience performance degradation when frozen and used in tracking \cite{xu2021rethinking}.  Additionally, as mentioned in \cite{wu2021progressive}, videos inherently possess supervisory signals which are the temporal co-associations among consecutive frames, and can provide rich supervision for representation learning. Therefore, approaches such as \cite{xu2021rethinking, li2022locality, yuan2022contextualized, gordon2020watching} have explored self-supervised correspondence learning specifically for videos. \cite{xu2021rethinking} utilizes image-level similarity framework to learn correspondences among frames from the same video, and showed a significant performance boost. Further, they incorporated a framework proposed in \cite{chen2021exploring} for correspondence learning without using negative samples, and found that incorporating negative samples during the learning phase had an adverse effect on the tracking. We integrate a similar correspondence learning scheme, but instead of pre-training the model, we train the correspondences alongside the regular training paradigm, and we show that this significantly improves the tracking performance. Our new \emph{transitive relation loss} is used as a correspondence learning paradigm to help bridge the visuo-temporal gap between the search frames and the templates. 

\section{Efficient Adaptation}
To tackle the dynamic distribution shifts during inference, we focus on Test-Time Adaptation (TTA). A recent popular approach, CoTTA~\cite{wang2022continual} uses data augmentation at test time to generate pseudo-labels and performs distillation for the image classification task. TENT \cite{wang2020tent} and EATA \cite{niu2022efficient} use model's test-time entropy to update only the BN learnable parameters. DUA \cite{mirza2022norm}, Momentum \cite{schneider2020improving}, IN \cite{pan2018two}, and AdaBN \cite{li2016revisiting} use backward-free BN-statistics updates to perform adaptation with maximal efficiency while showing considerably good accuracy. Most TTA approaches experience performance deterioration when used under real-world online applications, except BN-adaptation approaches as they are efficient and reliable~\cite{alfarra2023revisiting}. Therefore, we propose an efficient instance-level BN update strategy that continuously adapts the model to follow the dynamic visual changes of the target.
